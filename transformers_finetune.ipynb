{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Bert model\n",
    "\n",
    "- ref: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "\n",
    "- dataset: [Fake-News-Detection-Challenge-KDD-2020](https://huggingface.co/datasets/LittleFish-Coder/Fake-News-Detection-Challenge-KDD-2020)\n",
    "\n",
    "- pretrained model: [distilbert/distilbert-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased)\n",
    "\n",
    "- BERT architecture go through: [Coursera](https://www.coursera.org/learn/transformer-models-and-bert-model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install numpy pandas torch transformers datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "use the [`Fake-News-Detection-Challenge-KDD-2020`](https://huggingface.co/datasets/LittleFish-Coder/Fake-News-Detection-Challenge-KDD-2020) dataset from huggingface datasets library\n",
    "\n",
    "- 1: fake news\n",
    "- 0: real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.14k/1.14k [00:00<00:00, 5.50kB/s]\n",
      "Downloading data: 100%|██████████| 8.25M/8.25M [00:01<00:00, 7.22MB/s]\n",
      "Downloading data: 100%|██████████| 2.48M/2.48M [00:00<00:00, 3.38MB/s]\n",
      "Downloading data: 100%|██████████| 1.15M/1.15M [00:00<00:00, 2.13MB/s]\n",
      "Generating train split: 100%|██████████| 3490/3490 [00:00<00:00, 16005.05 examples/s]\n",
      "Generating validation split: 100%|██████████| 997/997 [00:00<00:00, 13708.40 examples/s]\n",
      "Generating test split: 100%|██████████| 499/499 [00:00<00:00, 19268.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset = load_dataset(\"LittleFish-Coder/Fake-News-Detection-Challenge-KDD-2020\", download_mode=\"reuse_cache_if_exists\", cache_dir=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 3490\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 997\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 499\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "print(f\"Dataset: {dataset}\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample\n",
      "Keys: dict_keys(['text', 'label', '__index_level_0__'])\n",
      "Text: UPDATE, WRITETHRU with more detail: Shortly before he was due to appear on ITV’s Good Morning Britain today, Ewan McGregor pulled out of the interview, citing comments made about this weekend’s Women’s March by host Piers Morgan. A supporter of President Donald Trump, Morgan yesterday on the program described some of the women who marched as “rabid feminists” and said he didn’t “see the point of the march(es)” which he called “generic” and “vacuous.”  On Twitter this morning, McGregor, who is out promoting Trainspotting sequel T2: Trainspotting, wrote, “Was going on Good Morning Britain, didn’t realise Piers Morgan was host. Won’t go on with him after his comments about #WomensMarch.”  Was going on Good Morning Britain, didn’t realise @piersmorgan was host. Won’t go on with him after his comments about #WomensMarch — Ewan McGregor (@mcgregor_ewan) January 24, 2017  On his Twitter account (whose timeline photo is of he and Trump), Morgan responded by saying McGregor is “just an actor after all.”  Sorry to hear that @mcgregor_ewan – you should be big enough to allow people different political opinions. You’re just an actor after all. — Piers Morgan (@piersmorgan) January 24, 2017  Morgan also tweeted he expected McGregor was reacting to his Monday column in the Daily Mail in which he called himself “a feminist,” but decried comments made by Madonna on Saturday saying, “I can’t abide the feminazis, the radical, extreme feminists like Madonna.”  On Tuesday, Morgan took to the middle-market UK tabloid to launch a vicious attack on McGregor where he referred to the actor as “unprofessional” and “disingenuous.”  “An actor who had contractually agreed to appear on a TV show to promote his new film pulls out at the last minute because he doesn’t like the political opinion of one of the presenters,” said Morgan.  Morgan said that after scrolling through McGregor’s Twitter feed, it “revealed a man absolutely enraged by both Donald Trump’s ascent to the presidency, and by Britain’s decision to leave the European Union.” He added: “His fury at the latter may strike some as slightly disingenuous given that McGregor himself quit the EU years ago to go and live the life of a pampered millionaire movie star in Hollywood. But I’ll leave others to decide whether that is hypocritical or not.”  The presenter continued his tirade by taking a swipe at celebrities who take the podium to make “political statements that remain unchallenged,” which he says was “typified by Meryl Streep’s extraordinarily pompous and elitist anti-Trump speech at the recent Golden Globes.”  Morgan even suggested that those who voted for Trump or for Brexit to “now boycott his movies” before criticizing McGregor for being “paedophile-loving” due to the fact he worked on Roman Polanski’s film The Ghostwriter.  A spokesperson for Good Morning Britain told the BBC today that McGregor “came into GMB this morning to be interviewed about his new film but decided not to go ahead with it.”  Per the Guardian, Morgan said on the telecast this morning, “Sorry that Ewan McGregor’s not here. He couldn’t bear the thought of being on the sofa with me because he doesn’t agree with me about the women’s march… I have to agree with what an actor thinks about a particular issue because they’re actors. And as we know actors’ views are more important than anybody else’s.”  McGregor’s daughters had marched on Saturday.  I’m with you in spirit today women of the world. My daughters are marching. I’m so proud to see this extraordinary power. — Ewan McGregor (@mcgregor_ewan) January 21, 2017  The long-awaited Trainspotting sequel reunites director Danny Boyle with McGregor, Robert Carlyle, Jonny Lee Miller and Ewen Bremner. The world premiere was held earlier this week in Edinburgh with the UK release coming Friday via Sony.\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# quick look at the data\n",
    "first_train = train_dataset[0]\n",
    "print(f\"First training sample\")\n",
    "print(f\"Keys: {first_train.keys()}\")\n",
    "print(f\"Text: {first_train['text']}\")\n",
    "print(f\"Label: {first_train['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with Pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'UPDATE, WRITETHRU with more detail: Shortly before he was due to appear on ITV’s Good Morning Britain today, Ewan McGregor pulled out of the interview, citing comments made about this weekend’s Women’s March by host Piers Morgan. A supporter of President Donald Trump, Morgan yesterday on the program described some of the women who marched as “rabid feminists” and said he didn’t “see the point of the march(es)” which he called “generic” and “vacuous.”  On Twitter this morning, McGregor, who is out promoting Trainspotting sequel T2: Trainspotting, wrote, “Was going on Good Morning Britain, didn’t realise Piers Morgan was host. Won’t go on with him after his comments about #WomensMarch.”  Was going on Good Morning Britain, didn’t realise @piersmorgan was host. Won’t go on with him after his comments about #WomensMarch — Ewan McGregor (@mcgregor_ewan) January 24, 2017  On his Twitter account (whose timeline photo is of he and Trump), Morgan responded by saying McGregor is “just an actor after all.”  Sorry to hear that @mcgregor_ewan – you should be big enough to allow people different political opinions. You’re just an actor after all. — Piers Morgan (@piersmorgan) January 24, 2017  Morgan also tweeted he expected McGregor was reacting to his Monday column in the Daily Mail in which he called himself “a feminist,” but decried comments made by Madonna on Saturday saying, “I can’t abide the feminazis, the radical, extreme feminists like Madonna.”  On Tuesday, Morgan took to the middle-market UK tabloid to launch a vicious attack on McGregor where he referred to the actor as “unprofessional” and “disingenuous.”  “An actor who had contractually agreed to appear on a TV show to promote his new film pulls out at the last minute because he doesn’t like the political opinion of one of the presenters,” said Morgan.  Morgan said that after scrolling through McGregor’s Twitter feed, it “revealed a man absolutely enraged by both Donald Trump’s ascent to the presidency, and by Britain’s decision to leave the European Union.” He added: “His fury at the latter may strike some as slightly disingenuous given that McGregor himself quit the EU years ago to go and live the life of a pampered millionaire movie star in Hollywood. But I’ll leave others to decide whether that is hypocritical or not.”  The presenter continued his tirade by taking a swipe at celebrities who take the podium to make “political statements that remain unchallenged,” which he says was “typified by Meryl Streep’s extraordinarily pompous and elitist anti-Trump speech at the recent Golden Globes.”  Morgan even suggested that those who voted for Trump or for Brexit to “now boycott his movies” before criticizing McGregor for being “paedophile-loving” due to the fact he worked on Roman Polanski’s film The Ghostwriter.  A spokesperson for Good Morning Britain told the BBC today that McGregor “came into GMB this morning to be interviewed about his new film but decided not to go ahead with it.”  Per the Guardian, Morgan said on the telecast this morning, “Sorry that Ewan McGregor’s not here. He couldn’t bear the thought of being on the sofa with me because he doesn’t agree with me about the women’s march… I have to agree with what an actor thinks about a particular issue because they’re actors. And as we know actors’ views are more important than anybody else’s.”  McGregor’s daughters had marched on Saturday.  I’m with you in spirit today women of the world. My daughters are marching. I’m so proud to see this extraordinary power. — Ewan McGregor (@mcgregor_ewan) January 21, 2017  The long-awaited Trainspotting sequel reunites director Danny Boyle with McGregor, Robert Carlyle, Jonny Lee Miller and Ewen Bremner. The world premiere was held earlier this week in Edinburgh with the UK release coming Friday via Sony.',\n",
       " 'labels': ['real', 'fake'],\n",
       " 'scores': [0.7533394694328308, 0.2466605007648468]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(first_train['text'], candidate_labels=[\"real\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess (Tokenize)\n",
    "The next step is to load a [`DistilBERT`](https://huggingface.co/distilbert/distilbert-base-uncased) tokenizer to preprocess the `text` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERT’s maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets map function. \n",
    "\n",
    "You can speed up map by setting `batched=True` to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3490/3490 [00:16<00:00, 213.71 examples/s]\n",
      "Map: 100%|██████████| 997/997 [00:04<00:00, 203.21 examples/s]\n",
      "Map: 100%|██████████| 499/499 [00:02<00:00, 222.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, the dataset will contain the original text and the following attributes that DistilBERT uses as input:\n",
    "\n",
    "- `input_ids`: The token indices in the vocabulary\n",
    "- `attention_mask`: Which parts of the sequence DistilBERT should pay attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokenized sample\n",
      "Keys: dict_keys(['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])\n",
      "Input IDs: [101, 10651, 1010, 4339, 2705, 6820, 2007, 2062, 6987, 1024, 3859, 2077, 2002, 2001, 2349, 2000, 3711, 2006, 11858, 1521, 1055, 2204, 2851, 3725, 2651, 1010, 1041, 7447, 23023, 2766, 2041, 1997, 1996, 4357, 1010, 8951, 7928, 2081, 2055, 2023, 5353, 1521, 1055, 2308, 1521, 1055, 2233, 2011, 3677, 16067, 5253, 1012, 1037, 10129, 1997, 2343, 6221, 8398, 1010, 5253, 7483, 2006, 1996, 2565, 2649, 2070, 1997, 1996, 2308, 2040, 9847, 2004, 1523, 10958, 17062, 10469, 2015, 1524, 1998, 2056, 2002, 2134, 1521, 1056, 1523, 2156, 1996, 2391, 1997, 1996, 2233, 1006, 9686, 1007, 1524, 2029, 2002, 2170, 1523, 12391, 1524, 1998, 1523, 12436, 10841, 3560, 1012, 1524, 2006, 10474, 2023, 2851, 1010, 23023, 1010, 2040, 2003, 2041, 7694, 4499, 11008, 3436, 8297, 1056, 2475, 1024, 4499, 11008, 3436, 1010, 2626, 1010, 1523, 2001, 2183, 2006, 2204, 2851, 3725, 1010, 2134, 1521, 1056, 19148, 16067, 5253, 2001, 3677, 1012, 2180, 1521, 1056, 2175, 2006, 2007, 2032, 2044, 2010, 7928, 2055, 1001, 2308, 26212, 11140, 1012, 1524, 2001, 2183, 2006, 2204, 2851, 3725, 1010, 2134, 1521, 1056, 19148, 1030, 16067, 5302, 16998, 2001, 3677, 1012, 2180, 1521, 1056, 2175, 2006, 2007, 2032, 2044, 2010, 7928, 2055, 1001, 2308, 26212, 11140, 1517, 1041, 7447, 23023, 1006, 1030, 23023, 1035, 1041, 7447, 1007, 2254, 2484, 1010, 2418, 2006, 2010, 10474, 4070, 1006, 3005, 17060, 6302, 2003, 1997, 2002, 1998, 8398, 1007, 1010, 5253, 5838, 2011, 3038, 23023, 2003, 1523, 2074, 2019, 3364, 2044, 2035, 1012, 1524, 3374, 2000, 2963, 2008, 1030, 23023, 1035, 1041, 7447, 1516, 2017, 2323, 2022, 2502, 2438, 2000, 3499, 2111, 2367, 2576, 10740, 1012, 2017, 1521, 2128, 2074, 2019, 3364, 2044, 2035, 1012, 1517, 16067, 5253, 1006, 1030, 16067, 5302, 16998, 1007, 2254, 2484, 1010, 2418, 5253, 2036, 1056, 28394, 3064, 2002, 3517, 23023, 2001, 24868, 2000, 2010, 6928, 5930, 1999, 1996, 3679, 5653, 1999, 2029, 2002, 2170, 2370, 1523, 1037, 10469, 1010, 1524, 2021, 11703, 11998, 7928, 2081, 2011, 11284, 2006, 5095, 3038, 1010, 1523, 1045, 2064, 1521, 1056, 11113, 5178, 1996, 10768, 22311, 5831, 2015, 1010, 1996, 7490, 1010, 6034, 10469, 2015, 2066, 11284, 1012, 1524, 2006, 9857, 1010, 5253, 2165, 2000, 1996, 2690, 1011, 3006, 2866, 24173, 2000, 4888, 1037, 13925, 2886, 2006, 23023, 2073, 2002, 3615, 2000, 1996, 3364, 2004, 1523, 4895, 21572, 7959, 28231, 2389, 1524, 1998, 1523, 4487, 7741, 2368, 8918, 1012, 1524, 1523, 2019, 3364, 2040, 2018, 27948, 2135, 3530, 2000, 3711, 2006, 1037, 2694, 2265, 2000, 5326, 2010, 2047, 2143, 8005, 2041, 2012, 1996, 2197, 3371, 2138, 2002, 2987, 1521, 1056, 2066, 1996, 2576, 5448, 1997, 2028, 1997, 1996, 25588, 1010, 1524, 2056, 5253, 1012, 5253, 2056, 2008, 2044, 28903, 2083, 23023, 1521, 1055, 10474, 5438, 1010, 2009, 1523, 3936, 1037, 2158, 7078, 18835, 2011, 2119, 6221, 8398, 1521, 1055, 16354, 2000, 1996, 8798, 1010, 1998, 2011, 3725, 1521, 1055, 3247, 2000, 2681, 1996, 2647, 2586, 1012, 1524, 2002, 2794, 1024, 1523, 2010, 8111, 2012, 1996, 3732, 2089, 4894, 2070, 2004, 3621, 4487, 7741, 2368, 8918, 2445, 2008, 23023, 2370, 8046, 1996, 7327, 2086, 3283, 2000, 2175, 1998, 2444, 1996, 2166, 1997, 102]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length: 512\n"
     ]
    }
   ],
   "source": [
    "# tokenized\n",
    "first_tokenized = tokenized_dataset[\"train\"][0]\n",
    "print(f\"First tokenized sample\")\n",
    "print(f\"Keys: {first_tokenized.keys()}\")\n",
    "print(f\"Input IDs: {first_tokenized['input_ids']}\")\n",
    "print(f\"Attention Mask: {first_tokenized['attention_mask']}\")\n",
    "print(f\"Length: {len(first_tokenized['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "# precision = evaluate.load(\"precision\")\n",
    "# recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    # pre = precision.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    # rec = recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    results = {\"accuracy\": acc['accuracy'], \"f1\": f1_score['f1']}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Finetune the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {1: \"fake\", 0: \"real\"}\n",
    "label2id = {\"fake\": 1, \"real\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! \n",
    "\n",
    "Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "output_dir = \"checkpoints\"\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.569913</td>\n",
       "      <td>0.708124</td>\n",
       "      <td>0.668871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.747242</td>\n",
       "      <td>0.742078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56, training_loss=0.5995587621416364, metrics={'train_runtime': 89.1929, 'train_samples_per_second': 78.257, 'train_steps_per_second': 0.628, 'total_flos': 924622442618880.0, 'train_loss': 0.5995587621416364, 'epoch': 2.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "trainer.save_model(f\"{output_dir}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics (on testing dataset)\n",
    "- Accuracy\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "val_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5129866600036621,\n",
       " 'eval_accuracy': 0.7472417251755266,\n",
       " 'eval_f1': 0.7420776730483101,\n",
       " 'eval_runtime': 4.57,\n",
       " 'eval_samples_per_second': 218.16,\n",
       " 'eval_steps_per_second': 1.751,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/littlefish/miniconda3/envs/huggingface/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "test_result = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5395698547363281,\n",
       " 'eval_accuracy': 0.7274549098196392,\n",
       " 'eval_f1': 0.7199946994304812,\n",
       " 'eval_runtime': 2.3559,\n",
       " 'eval_samples_per_second': 211.805,\n",
       " 'eval_steps_per_second': 1.698,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that you've finetuned a model, you can use it for inference!\n",
    "\n",
    "Grab some text you'd like to run inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: How great it would be to be able to go out on a date with a favorite superheroe. Probably for some of us our partners are our personal superheroes, but we know that deep down we dream of the God of thunder, beautiful spies stealing classified information, night watchmen, or women with pyrotechnic skills. At some point, everyone has wished to have a date with one of those characters or at least with one of the actors that played them.  But for some people, this dream has come true. Deep in their hearts everyone else envies them because they know it must be fun to be able to say that their husband, wife or even one of their parents has been defeating different villains under the spotlights of the Hollywood cameras. So, let's stop and think about it for a moment... if being a parent is already a challenge then try to picture being parents and superheroes at the same time. Anyone else would lose their minds! But Commissioner James Gordon once said \"[One has] to make a difference. A lot of time it won't be huge. It won't be visible even. But it will matter just the same\".  That's why today we will talk about those actors who have brought the world's favorite superheroes to life and have had to learn to balance their lives as parents and superheroes in the film industry.  20 Gal Gadot Is The Perfect Wonder Woman  Gal Gadot is an Israeli actress and model, who is best known for her role in films such as Fast and Furious, and for playing the role of Wonder Woman in the film adaptation of DC comics, participating in films such as Batman v Superman: Dawn of Justice (2016), Wonder Woman (2017) and Justice League (2017).  And according to Forbes, Wonder Woman has become the most successful movie of the D.C franchise, making $ 821.74 million at the box office. Gal, in her life outside of the cameras, is the wife of Yaron Versano, an Israeli businessman. The couple is married since 2008 and they have 2 children, Alma Versano who was born in 2011 and Maya Versano who was born in 2017.  19 People Don't Like Ben Affleck's Batman  Via Pinterest  Ben Affleck is an American actor who comes from a long career in the industry, acting in films like Good Will Hunting (1997), Armageddon (1998), and Argo (2012). But recently, Affleck was selected to give life to one of the most important superheroes of DC, Batman, after Christian Bale played the same character from 2005 to 2012. Unfortunately, Affleck did not have the same receptivity in the audience as Bale had. Some dislike him, others love him. In fact, Variety places Affleck among the worst actors who have played the bat on the big screen.  However, beyond being considered one of the worst Batman in history, Ben was married to actress Jennifer Garner from 2005 to 2017, with whom he has 3 children, Violet Affleck who was born in 2005, Seraphina Rose Elizabeth Affleck, who was born in 2009, and Samuel Garner Affleck who was born in 2012.  18 Scarlett Johansson's Daughter Says Her Mom's Job Is \"Superhero\"  Via gotceleb.com  Scarlett Johansson is an actress who began her acting career from an early age with small minor roles until she began to become notorious in her adulthood with roles in films like Lost in Translation (2003) with Billy Murray, which allowed her to establish herself as an actress in Hollywood. Scarlett, according to her biography in IMDb, was the best-paid actress between 2014 and 2016.  This American actress has an exotic and different beauty, which is why we are not surprised that she was chosen to play the role of the black widow in the Marvel franchise \"The Avengers\". Moreover, Scarlett revealed a funny fact during an interview on Ellen DeGeneres’ talk show, that her 4 years old daughter believes she is a real superhero. \"If you ask her what I do for a living she says, 'Mommy's a superhero'.\"  17 James McAvoy: Professor X  https://weheartit.com/entry/119923616  Raise your hand to who would like to be saved by James McAvoy? Have you seen those eyes? Impossible not to get lost in that blue. McAvoy is an actor of Scottish origin who is best known for his roles in films such as the Narnia franchise, Penelope (2007), and Atonement (2007), among others. Since 2011, McAvoy has joined the cast of the new generation of X-Men, a franchise based on Marvel comics. In this new generation, McAvoy plays a much younger version of the iconic Professor Charles Xavier.  Meanwhile, in his life outside the cameras, McAvoy enjoys his role as a father. Between 2006 and 2016 the actor was married to Anne-Marie Duff, an English actress 9 years older than him. The couple received their first child Brendan McAvoy in 2010, and have always maintained a conservative life, and keep their son away from the cameras, so there are not many photos of them.  16 Chris Pratt's Son Prefers Captain America  Via Daily Mail  Chris Pratt is an American actor who until a few years ago had only minor roles, and was mostly known for his role as Andy Dwyer in the NBC sitcom Parks and Recreation (2009-2015). But in 2014 he had the opportunity to join the Marvel franchise and play the role of Peter Quill / Star-Lord in the movie Guardians of the Galaxy. This role served to boost his acting career and obtain leading roles in films such as Jurassic World (2015) and Passengers (2016).  Moreover, Chris has a son with the well-known actress Anna Faris, with whom he was married from 2009 to 2017. The couple had a son in 2012, Jack Pratt, and currently, both have a good relationship and are co-parenting. Ironically, Jack is not a fan of his father's characters in action movies or superheroes. Jack has proven to be a great admirer of Captain America, and luckily, his dad knows Chris Evans.  15 Cobie Smulders A.K.A. Robin Scherbatsky  Via Pinteres  Cobie Smulders is a Canadian actress who achieved fame thanks to her role as Robin Scherbatsky on the comedy series How I Met Your Mother (2005-2014), which helped her later to obtain the role of Maria Hill, former director of S.H.I.E.L.D, in the films of the Avengers franchise, inspired by Marvel comics.  Cobie is married to the actor Taran Killam, best known for his work on television, participating in shows like The Amanda Show, and Saturday Night Live. The couple has been married since 2002 and they have two kids, 8-year-old Shaelyn Cado, and a 4-year-old child. But how cool it must be to be able to tell your classmates that your mom is fighting the villains in the world and is a friend of The Avengers.  14 Chris Hemsworth: God Of Thunder  Via Daily Mail  Okay, ladies! Let's be honest, we all envy Elsa Pataky, the actress and Spanish model, who also is the proud wife of Chris Hemsworth (AKA Thor) since 2010. The couple has 3 beautiful children, India Rose Hemsworth who was born in 2012, and the twins Tristan Hemsworth and Sasha Hemsworth that were born in 2014. He is also the big brother of Miley Cyrus's boyfriend, Liam Hemsworth.  Chris is an actor of Australian origin who had some minor roles in Hollywood, despite having been well known in Australia for participating in television series such as Guinevere Jones and Fergus McPhail. But his jump to fame came when he was selected in 2011 to play the Thunder God Thor.  13 Zoe Saldana: Avatar And Guardians Of The Galaxy  Via Daily Mail  Zoe Saldana is an American actress and dancer who has been acting in Hollywood for a long time and is best known for her roles in films such as Avatar (2010), the Star Trek franchise, and recently, she joined the group of actors who belong to Marvel's superheroes inspired by the comics. Saldana, since 2014 has been playing the role of Gamora as part of the team of Guardians of the Galaxy and The Avengers.  Zoe, besides saving the world from terrible villains, is the mother of 3 children with her husband Marco Perego, an Italian artist. The couple has been married since 2013. And according to Page Six, they met in New York, but it is unknown exactly how and where they met because the couple is often too reserved with their personal lives and avoid exposing themselves too much under the spotlight.  12 Gwyneth Paltrow Is Iron Man's Girlfriend  Via People  The American actress and singer, Gwyneth Kate Paltrow, known for her roles in films such as Shakespeare in Love (1998), or for being the girlfriend of Iron Man in the films inspired by the Marvel comics. But during Iron Man 3, Pepper (Gwyneth) did what many women have long hoped for the superhero girlfriends, she defended her boyfriend Tony Stark against the villain.  In her real life, Gwyneth was married to Chris Martin, the lead singer of the British band Coldplay, from 2003 till 2016. The couple had two children, Apple who was born in 2004, and Moses who was born in 2006. Gwyneth is now engaged to Brad Falchuk, an American screenwriter, and producer, while Chris is dating, actress Dakota Johnson.  11 Bradley Cooper Voices Rocket Raccoon  Via Page Six  Bradley Cooper is an American actor and producer, who is known for films such as Silver Linings Playbook (2012), American Hustle (2013), and American Sniper (2014). But currently he's been working as one of the superheroes that belong to the Galaxy Guardians team, and if you had not noticed, the handsome Cooper is the one who gives voice to Rocket Raccoon.  But regarding his personal life, Bradley has been dating Irina Shayk a Russian model and one of the Victoria Secret's Angels. In 2017, the couple welcomed their first daughter whom they named Lea De Seine Shayk Cooper, and since the birth of their daughter the couple seems to be more united than ever, but so far it is unknown if they plan to take the big step and get married.  10 Benedict Cumberbatch Is The Perfect Dr. Strange  https://www.bustle.com/articles/47648-17-times-benedict-cumberbatch-expressed-wanting-a-family-more-than-anything  Benedict Cumberbatch is an actor of English origin known for his roles in films such as Atonement, 12 Years to Slave, and the Star Trek franchise. In 2015 the actor got the role of one of Marvel's superheroes, Doctor Strange, who had his own film and soon after joined the plot of The Avengers, due to his ability to time travel.  According to The Sun UK, Benedict met his future wife, Sophie Hunter, in the thriller Burlesque Fairytales in 2009, but it was not until 2013 that Cumberbatch reconnected with Sophie and they started dating. Sophie is a theater director, playwright and former performer. The couple married in 2015 and currently have two children, Christopher Carlton, who was born in June 2015, and Hal Auden, who was born in March 2017.  9 Hugh Jackman Was Wolverine For 17 Years  http://www.dailymail.co.uk/tvshowbiz/article-2042029/Hugh-Jackman-opens-painful-road-parenthood.html  Hugh Jackman is an Australian actor and is considered one of the established actors in the film industry. We all remember the actor thanks to films like Van Helsing (2004), Les Misérables (2012), The Greatest Showman (2017). But probably his most memorable character has been Wolverine (Logan) from the universe of Marvel, X-Men, being one of the favorite characters of the audience.  Since 1996, Hugh has been married to Deborra-Lee Furness, another actress of Australian origin. The couple, unfortunately, could never conceive their own children, but that did not stop them at the time of wanting to become parents, which is why they decided to adopt. Both have two children, Oscar Maximilian who was born in 2000 and Ava Eliot who was born in 2005.  8 Evangeline Lilly Is No Longer Lost  Via dailymail.co.uk  Evangeline Lilly is a Canadian actress who stole our hearts when she played the role of Kate in the television series Lost (2004-2010). She also starred in other films such as The Hurt Locker (2008), Real Steel (2011) and The Hobbit film series (2013-14). But recently, Evangeline was selected to play the role of Hope van Dyne, the romantic interest of the superhero Ant-Man, but Hope is not only the girlfriend of the superhero who is just there to be rescued, but she also has a role quite prominent in the movies of Marvel, so she also has superpowers.  However, in real life, Evangeline has a relationship with Norman Kali, whom the actress met during the filming of Lost. Currently, the couple has 2 children, Kahekili Kali who was born in 2011 and in 2015 the couple received their second child.  7 Jason Momoa: Aquaman Has Never Looked So Good  Jason Momoa is an American actor and producer who is best known for his role in the television series Game of Thrones, where he played Khal Drogo, leader of the Dothraki tribe and husband of the mother of dragons, Daenerys Targaryen. And it was thanks to that role that Jason subsequently got the role of Aquaman in the movie Justice League (2017).  Jason in real life is married to the actress Lisa Bonet, whom the actor has confessed has been in love since he was a child. “I was 8-years-old and I saw her on the TV and I was like, ‘Mommy, I want that one,’” he told during the talk show of James Corden. “I was like, ‘I’m going to stalk you for the rest of my life, and I’m going to get you.’ I’m a full-fledged stalker.” The couple married in 2017 and have two children.  6 Paul Rudd Is The \"Smallest\" Superhero  Via Pinterest  Do you guys remember Josh from the movie \"Clueless\" from 1995? Well, that role was played by Paul Rudd, and since that kiss on the stairs with Cher, I must admit that he stole my heart. Since then Paul has made other films but almost always playing secondary roles, but his great opportunity came when he was offered the role of Ant-Man for the 2015 Marvel movie, and since then we have seen him in other films of the Marvel universe as Captain America: Civil War (2016) and the Avengers (2018).  Paul has been married since 2003 to Julie Yaeger, a publicist. According to Bustle, the couple met in New York after Paul was recommended to get a publicist. They currently have two children, Jack Sullivan Rudd who was born in 2006 and Darby Rudd who was born in 2010.  5 Ryan Reynolds Has Been Two Different Superheroes  Via Pinterest  Ryan Reynolds is a Canadian actor that we remember from films like, Blade: Trinity (2004), Just Friends (2005), X-Men Origins: Wolverine (2009) and The Proposal (2009). Reynolds already had time playing superheroes as Deadpool in the version of X-Men and Green Lantern, but actually earned the respect of the audience and the critics after playing again the role of Deadpool, under the direction of Tim Miller, becoming the most successful adult rated movie ever.  Ryan who was previously married to Scarlett Johansson found love again when he met the actress Blake Lively with whom he has been married since 2012. The couple has two daughters, James Reynolds who was born in 2014 and Inez Reynolds who was born in 2016.  4 Halle Berry: Bond Girl And X-Man  Via Pinterest  Halle Berry is an American actress who has stolen the attention of the audience since her first films. Berry has starred in films such as The Rich Man's Wife (1996), Monster's Ball (2001), 007 (2002), and Kingsman: The Golden Circle (2017), among others. But we all remember Halle for her role as Storm of the X-Men franchise in early 2000.  In 2005, Halle met the Canadian model Gabriel Aubry during a photo shoot, both started dating and in 2008 they had their first daughter Nahla, and two years later they announced their separation. Subsequently, Halle got married for the third time to the French actor, Olivier Martinez, whom she met during the filming of the movie Dark Tide. The couple was married from 2013 to 2016, during their marriage they welcomed a son called Maceo Robert Martinez who was born in 2013.  3 Selma Blair Hasn't Been A Superhero In A Long Time  Via dailymail.co.uk  Selma Blair is an American actress known for her roles in films such as Cruel Intentions (1999), Legally Blonde (2001) and The Sweetest Thing (2002). Although, comics fans probably remember her more for her role as Liz Sherman, a pupil of B.P.R.D., in the Hellboy film franchise from 2004 to 2008.  But in real life, Selma has a calmer life, beyond fighting villains in the comic book movie world. The actress was married to Ahmet Zappa from 2004 to 2006, but it was when she met Jason Bleick, a fashion designer, that Selma got pregnant with her son Arthur Saint Bleick who was born in 2011. Fourteen months after the birth of her son, the couple split.  2 Ming-Na Wen Was Mulan Before Joining S.H.E.I.L.D.  Via Zimbio  Ming-Na Wen is an American Chinese actress, best known for her role as Melinda May in the ABC action drama series Agents of S.H.I.E.L.D. This actress has also lent her voice to films such as Mulan and Mulan II (as Fa Mulan), Sofia the First in the Disney animated series, and she reprised her role as Mulan in the new film Ralph Breaks the Internet: Wreck-It Ralph 2  This actress has tried to keep her personal life out of the spotlights of the cameras. After a failed first marriage, Ming-Na Wen seemed to find love again and married the actor Eric Michael Zee in 1993. In the year 2000 the couple received their daughter Michaela Zee and in 2005 they welcomed their second child, Cooper Dominic Zee.  1 Dan Stevens Is A Long Way From Downton Abbey  Via Pinterest  Dan Stevens is an English actor best known for his role as Matthew Crawley, from the Downton Abbey series. Later, Dan obtained roles in films such as The Fifth Estate (2013), Night at the Museum: Secret of the Tomb (2014), and Beauty and the Beast (2017). But lately, the actor has become more notorious thanks to his role in the television series Legion, based on the character of Marvel Comics David Haller, and the tv show is connected to the series of films X-Men.  Dan according to his biography in IMDb is married to Susie Stevens since 2009. The couple currently has 3 children, Willow Stevens who was born in 2009, Aubrey Stevens who was born in 2012 and Eden Stevens who was born in 2016.  References: huffingtonpost.com, contactmusic.com, bustle.com, pagesix.com, forbes.com, variety.com, dailymail.co.uk, etonline.com, wired.co.uk, imdb.com\n",
      "Real Label: 0\n"
     ]
    }
   ],
   "source": [
    "text = test_dataset[0][\"text\"]\n",
    "real_label = test_dataset[0][\"label\"]\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Real Label: {real_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'fake', 'score': 0.6022790670394897}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=f\"{output_dir}/best_model\", truncation=True, device=device)\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually replicate the results of the `pipeline` if you'd like:\n",
    "\n",
    "Tokenize the text and return PyTorch tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the tokenizer and model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Input: {'input_ids': tensor([[  101,  2129,  2307,  2009,  2052,  2022,  2000,  2022,  2583,  2000,\n",
      "          2175,  2041,  2006,  1037,  3058,  2007,  1037,  5440, 16251,  2063,\n",
      "          1012,  2763,  2005,  2070,  1997,  2149,  2256,  5826,  2024,  2256,\n",
      "          3167, 16251,  2229,  1010,  2021,  2057,  2113,  2008,  2784,  2091,\n",
      "          2057,  3959,  1997,  1996,  2643,  1997,  8505,  1010,  3376, 16794,\n",
      "         11065,  6219,  2592,  1010,  2305,  3422,  3549,  1010,  2030,  2308,\n",
      "          2007,  1052, 12541, 12184,  2818,  8713,  4813,  1012,  2012,  2070,\n",
      "          2391,  1010,  3071,  2038,  6257,  2000,  2031,  1037,  3058,  2007,\n",
      "          2028,  1997,  2216,  3494,  2030,  2012,  2560,  2007,  2028,  1997,\n",
      "          1996,  5889,  2008,  2209,  2068,  1012,  2021,  2005,  2070,  2111,\n",
      "          1010,  2023,  3959,  2038,  2272,  2995,  1012,  2784,  1999,  2037,\n",
      "          8072,  3071,  2842,  4372, 25929,  2068,  2138,  2027,  2113,  2009,\n",
      "          2442,  2022,  4569,  2000,  2022,  2583,  2000,  2360,  2008,  2037,\n",
      "          3129,  1010,  2564,  2030,  2130,  2028,  1997,  2037,  3008,  2038,\n",
      "          2042,  6324,  2367, 16219,  2104,  1996, 17763,  2015,  1997,  1996,\n",
      "          5365,  8629,  1012,  2061,  1010,  2292,  1005,  1055,  2644,  1998,\n",
      "          2228,  2055,  2009,  2005,  1037,  2617,  1012,  1012,  1012,  2065,\n",
      "          2108,  1037,  6687,  2003,  2525,  1037,  4119,  2059,  3046,  2000,\n",
      "          3861,  2108,  3008,  1998, 16251,  2229,  2012,  1996,  2168,  2051,\n",
      "          1012,  3087,  2842,  2052,  4558,  2037,  9273,   999,  2021,  5849,\n",
      "          2508,  5146,  2320,  2056,  1000,  1031,  2028,  2038,  1033,  2000,\n",
      "          2191,  1037,  4489,  1012,  1037,  2843,  1997,  2051,  2009,  2180,\n",
      "          1005,  1056,  2022,  4121,  1012,  2009,  2180,  1005,  1056,  2022,\n",
      "          5710,  2130,  1012,  2021,  2009,  2097,  3043,  2074,  1996,  2168,\n",
      "          1000,  1012,  2008,  1005,  1055,  2339,  2651,  2057,  2097,  2831,\n",
      "          2055,  2216,  5889,  2040,  2031,  2716,  1996,  2088,  1005,  1055,\n",
      "          5440, 16251,  2229,  2000,  2166,  1998,  2031,  2018,  2000,  4553,\n",
      "          2000,  5703,  2037,  3268,  2004,  3008,  1998, 16251,  2229,  1999,\n",
      "          1996,  2143,  3068,  1012,  2322, 14891, 11721, 27364,  2003,  1996,\n",
      "          3819,  4687,  2450, 14891, 11721, 27364,  2003,  2019,  5611,  3883,\n",
      "          1998,  2944,  1010,  2040,  2003,  2190,  2124,  2005,  2014,  2535,\n",
      "          1999,  3152,  2107,  2004,  3435,  1998,  9943,  1010,  1998,  2005,\n",
      "          2652,  1996,  2535,  1997,  4687,  2450,  1999,  1996,  2143,  6789,\n",
      "          1997,  5887,  5888,  1010,  8019,  1999,  3152,  2107,  2004,  8942,\n",
      "          1058, 10646,  1024,  6440,  1997,  3425,  1006,  2355,  1007,  1010,\n",
      "          4687,  2450,  1006,  2418,  1007,  1998,  3425,  2223,  1006,  2418,\n",
      "          1007,  1012,  1998,  2429,  2000, 10822,  1010,  4687,  2450,  2038,\n",
      "          2468,  1996,  2087,  3144,  3185,  1997,  1996,  1040,  1012,  1039,\n",
      "          6329,  1010,  2437,  1002,  6445,  2487,  1012,  6356,  2454,  2012,\n",
      "          1996,  3482,  2436,  1012, 14891,  1010,  1999,  2014,  2166,  2648,\n",
      "          1997,  1996,  8629,  1010,  2003,  1996,  2564,  1997,  8038,  4948,\n",
      "         18601,  3630,  1010,  2019,  5611,  6883,  1012,  1996,  3232,  2003,\n",
      "          2496,  2144,  2263,  1998,  2027,  2031,  1016,  2336,  1010, 11346,\n",
      "         18601,  3630,  2040,  2001,  2141,  1999,  2249,  1998,  9815, 18601,\n",
      "          3630,  2040,  2001,  2141,  1999,  2418,  1012,  2539,  2111,  2123,\n",
      "          1005,  1056,  2066,  3841, 21358, 21031,  3600,  1005,  1055,  8942,\n",
      "          3081,  9231,  3334,  4355,  3841, 21358, 21031,  3600,  2003,  2019,\n",
      "          2137,  3364,  2040,  3310,  2013,  1037,  2146,  2476,  1999,  1996,\n",
      "          3068,  1010,  3772,  1999,  3152,  2066,  2204,  2097,  5933,  1006,\n",
      "          2722,  1007,  1010,  2849, 18655,  5280,  1006,  2687,  1007,  1010,\n",
      "          1998, 12098,  3995,  1006,  2262,  1007,  1012,  2021,  3728,  1010,\n",
      "         21358,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{output_dir}/best_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "print(f\"Input keys: {inputs.keys()}\")\n",
    "print(f\"Input: {inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass your inputs to the model and return the `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-0.3234,  0.0916]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"{output_dir}/best_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "print(f\"Logits: {logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fake'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forgery-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
